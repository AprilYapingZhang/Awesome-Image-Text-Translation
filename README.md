# awsome-image-text-translation
a curated list of image text translation

------
## Text Image Translation
### 2023
- Shaolin Zhu∗, Shangjie Li∗, Yikun Lei, Deyi xiong†. [PEIT: Bridging the Modality Gap with Pre-trained Models for End-to-End
Image Translation](https://aclanthology.org/2023.acl-long.751.pdf)[C]. ACL 2023; **code**:[[code](https://github.com/lishangjie1/PEIT)]
- Zhibin Lan, Jiawei Yu, Xiang Li, Wen Zhang, Jian Luan, Bin Wang, Degen Huang, Jinsong Su. [Exploring Better Text Image Translation with Multimodal Codebook](https://arxiv.org/pdf/2305.17415.pdf)[C]. ACL 2023.  **code**:[[code](https://github.com/DeepLearnXMU/mc_tit)]. 
- Cong Ma; Xu Han; Linghui Wu; Yaping Zhang; Yang Zhao; Yu Zhou; Chengqing Zong. [Modal Contrastive Learning based End-to-End Text Image Machine Translation](https://ieeexplore.ieee.org/abstract/document/10284997)[J]. TASLP 2023.
  
- Multi-teacher Knowledge Distillation for End-to-End Text Image Machine Translation. ICDAR 2023.
- E2TIMT: Efficient and Effective Modal Adapter for Text Image Machine Translation. ICDAR 2023.
### 2022
- Cong Ma, Yaping Zhang, Mei Tu, Xu Han, Linghui Wu, Yang Zhao, Yu Zhou. [Improving End-to-End Text Image Translation From the Auxiliary Text Translation Task](https://arxiv.org/pdf/2210.03887.pdf)[C]. ICPR 2022.  **code**:[[code](https://github.com/EriCongMa/E2E_TIT_With_MT)]. 



## Document Image Translation 
### 2023


### 2021
- Ryota Hinami, Shonosuke Ishiwatari, Kazuhiko Yasuda, Yusuke Matsui. [Towards Fully Automated Manga Translation](https://arxiv.org/pdf/2012.14271.pdf). AAAI 2021. 



- `Benchmark Datasets`

|Dataset| Discription | Competition Paper |
|---|---|----
|[ICDAR 2015](http://rrc.cvc.uab.es/)| 1000 training images and 500 testing images|`paper`  [![link](https://www.lds.org/bc/content/shared/content/images/gospel-library/manual/10735/paper-icon_1150845_tmb.jpg)](http://rrc.cvc.uab.es/files/Robust-Reading-Competition-Karatzas.pdf)|
|[ICDAR 2013](http://dagdata.cvc.uab.es/icdar2013competition/)| 229 training images and 233 testing images |`paper`  [![link](https://www.lds.org/bc/content/shared/content/images/gospel-library/manual/10735/paper-icon_1150845_tmb.jpg)](http://dagdata.cvc.uab.es/icdar2013competition/files/icdar2013_competition_report.pdf)|
|[ICDAR 2011](http://robustreading.opendfki.de/trac/)| 229 training images and 255 testing images |`paper`  [![link](https://www.lds.org/bc/content/shared/content/images/gospel-library/manual/10735/paper-icon_1150845_tmb.jpg)](http://www.iapr-tc11.org/archive/icdar2011/fileup/PDF/4520b491.pdf)|
|[ICDAR 2005](http://www.iapr-tc11.org/mediawiki/index.php/ICDAR_2005_Robust_Reading_Competitions)| 1001 training images and 489 testing images |`paper`  [![link](https://www.lds.org/bc/content/shared/content/images/gospel-library/manual/10735/paper-icon_1150845_tmb.jpg)](http://www.academia.edu/download/30700479/10.1.1.96.4332.pdf)|
|[ICDAR 2003](http://www.iapr-tc11.org/mediawiki/index.php/ICDAR_2003_Robust_Reading_Competitions)| 181 training images and 251 testing images(word level and character level) |`paper`  [![link](https://www.lds.org/bc/content/shared/content/images/gospel-library/manual/10735/paper-icon_1150845_tmb.jpg)](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.332.3461&rep=rep1&type=pdf)|


---











